{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"UNSLOTH_COMPILE_DISABLE\"] = \"1\"\n",
        "os.environ[\"UNSLOTH_DISABLE_FAST_GENERATION\"] = \"1\""
      ],
      "metadata": {
        "id": "FvksWhaL8hzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = \"/content/drive/MyDrive/smart_ecom_assistant\"\n",
        "!mkdir -p \"$BASE_DIR\""
      ],
      "metadata": {
        "id": "_IXSw8he9CTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip! install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "1mzm_Rpc8G-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw6K44nKM3D9"
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    \"model_name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "    \"max_seq_length\": 768,\n",
        "    \"epochs\": 1,\n",
        "    \"batch_size\": 1,\n",
        "    \"accum_steps\": 16,\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"warmup_ratio\": 0.05,\n",
        "    \"weight_decay\": 0.01,\n",
        "\n",
        "    \"train_path\": \"/content/drive/MyDrive/My_Machine_Learning/smart_E-commerce_assistant/samples/train_pairs_train.jsonl\",\n",
        "    \"val_path\":   \"/content/drive/MyDrive/My_Machine_Learning/smart_E-commerce_assistant/samples/train_pairs_val.jsonl\",\n",
        "    \"output_dir\": \"/content/drive/MyDrive/My_Machine_Learning/smart_E-commerce_assistant/models/lora-shopping-assistant\",\n",
        "\n",
        "    \"seed\": 42,\n",
        "}\n",
        "CONFIG\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r                          = 16,\n",
        "    target_modules             = [\"q_proj\", \"v_proj\"],\n",
        "    lora_alpha                 = 32,\n",
        "    lora_dropout               = 0.05,\n",
        "    bias                       = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state               = CONFIG[\"seed\"],\n",
        "    use_rslora                 = False,\n",
        ")"
      ],
      "metadata": {
        "id": "051ARi359GD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "bf16_supported = is_bfloat16_supported()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir                  = CONFIG[\"output_dir\"],\n",
        "    num_train_epochs            = CONFIG[\"epochs\"],\n",
        "    per_device_train_batch_size = CONFIG[\"batch_size\"],\n",
        "    gradient_accumulation_steps = CONFIG[\"accum_steps\"],\n",
        "    learning_rate               = CONFIG[\"learning_rate\"],\n",
        "    weight_decay                = CONFIG[\"weight_decay\"],\n",
        "    warmup_ratio                = CONFIG[\"warmup_ratio\"],\n",
        "    logging_steps               = 10,\n",
        "    logging_first_step          = True,\n",
        "    save_steps                  = 200,\n",
        "    eval_strategy               = \"epoch\",\n",
        "    save_total_limit            = 2,\n",
        "    bf16                        = bf16_supported,\n",
        "    fp16                        = not bf16_supported,\n",
        "    optim                       = \"paged_adamw_8bit\",\n",
        "    report_to                   = \"none\",\n",
        ")"
      ],
      "metadata": {
        "id": "U4DC103k95_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_batch(batch):\n",
        "    enc = tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation   = True,\n",
        "        max_length   = MAX_SEQ_LENGTH,\n",
        "        padding      = \"max_length\",\n",
        "    )\n",
        "    labels = []\n",
        "    pad_id = tokenizer.pad_token_id\n",
        "\n",
        "    for seq in enc[\"input_ids\"]:\n",
        "        labels.append([\n",
        "            (tok if tok != pad_id else -100)\n",
        "            for tok in seq\n",
        "        ])\n",
        "\n",
        "    enc[\"labels\"] = labels\n",
        "    return enc\n",
        "\n",
        "tokenized = dataset.map(\n",
        "    tokenize_batch,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names,\n",
        ")\n",
        "\n",
        "train_dataset = tokenized[\"train\"]\n",
        "eval_dataset  = tokenized[\"validation\"]\n",
        "\n",
        "print(train_dataset[0].keys())"
      ],
      "metadata": {
        "id": "auOnwf9Y96Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bf16_supported = is_bfloat16_supported()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir                  = CONFIG[\"output_dir\"],\n",
        "    num_train_epochs            = CONFIG[\"epochs\"],\n",
        "    per_device_train_batch_size = CONFIG[\"batch_size\"],\n",
        "    gradient_accumulation_steps = CONFIG[\"accum_steps\"],\n",
        "    learning_rate               = CONFIG[\"learning_rate\"],\n",
        "    logging_steps               = 5,\n",
        "    logging_first_step          = True,\n",
        "    save_steps                  = 200,\n",
        "    eval_strategy               = \"epoch\",\n",
        "    save_total_limit            = 2,\n",
        "    bf16                        = bf16_supported,\n",
        "    fp16                        = not bf16_supported,\n",
        "    optim                       = \"paged_adamw_8bit\",\n",
        "    lr_scheduler_type           = \"cosine\",\n",
        "    warmup_ratio                = 0.05,\n",
        "    report_to                   = \"none\",\n",
        ")"
      ],
      "metadata": {
        "id": "kKnQ_NIf_U-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model         = model,\n",
        "    args          = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset  = eval_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Q7CEJx8y_X9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = f'{CONFIG[\"output_dir\"]}_final'\n",
        "import os\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "print(\"Saved to:\", save_dir)"
      ],
      "metadata": {
        "id": "feq4FUjz_aLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/lora-shopping-assistant-final.zip \"$save_dir\"\n",
        "from google.colab import files\n",
        "files.download(\"/content/lora-shopping-assistant-final.zip\")"
      ],
      "metadata": {
        "id": "WJq0E77H_cON"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}